- [ ] **Kingma, D. P. & Welling, M. (2013).** *Auto-Encoding Variational Bayes.*  
  Introduced the VAE framework and laid the foundation for efficient stochastic variational inference in deep generative models.

- [ ] **Rezende, D. J., Mohamed, S., & Wierstra, D. (2014).** *Stochastic Backpropagation and Approximate Inference in Deep Generative Models.*  
  Developed an alternative formulation and inference procedure, highlighting the potential of VAEs in learning complex distributions.

- [ ] **Kingma, D. P. et al. (2014).** *Semi-Supervised Learning with Deep Generative Models.*  
  Extended the VAE framework to leverage both labeled and unlabeled data, influencing subsequent work in semi-supervised learning.

- [ ] **Burda, Y., Grosse, R., & Salakhutdinov, R. (2015).** *Importance Weighted Autoencoders.*  
  Introduced the importance weighted bound for VAEs, improving the tightness of the variational bound and model performance.

- [ ] **Rezende, D. J. & Mohamed, S. (2015).** *Variational Inference with Normalizing Flows.*  
  Enhanced the expressiveness of the variational posterior by applying a sequence of invertible transformations.

- [ ] **Kingma, D. P. et al. (2016).** *Improved Variational Inference with Inverse Autoregressive Flow.*  
  Proposed inverse autoregressive flows to further refine the flexibility of the posterior approximations in VAEs.

- [ ] **Chen, X. et al. (2016).** *Variational Lossy Autoencoder.*  
  Investigated the balance between reconstruction fidelity and latent compression, influencing models for lossy data representations.

- [ ] **Gulrajani, I. et al. (2016).** *PixelVAE: A Latent Variable Model for Natural Images.*  
  Combined VAE latent variables with powerful autoregressive decoders, advancing generative image modeling.

- [ ] **Higgins, I. et al. (2017).** *β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework.*  
  Focused on disentanglement of latent representations by modifying the VAE objective with a β hyperparameter.

- [ ] **Mescheder, L. et al. (2017).** *Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks.*  
  Blended the strengths of VAEs and GANs to yield models that leverage both adversarial training and variational inference.

- [ ] **Tolstikhin, I. et al. (2018).** *Wasserstein Auto-Encoders.*  
  Introduced a new framework that connects optimal transport with generative modeling, providing an alternative to the standard VAE formulation.

- [ ] **Kim, H. & Mnih, A. (2018).** *Semi-Amortized Variational Autoencoders.*  
  Addressed challenges in the amortization gap by combining iterative refinement with amortized inference in VAEs.

- [ ] **Kim, H. & Mnih, A. (2018).** *Disentangling by Factorising.*  
  Proposed methods to enforce disentangled representations in the latent space, building on the ideas behind β-VAE.

- [ ] **Zhao, S. et al. (2017).** *InfoVAE: Balancing Learning and Inference in Variational Autoencoders.*  
  Offered an alternative VAE objective that balances the trade-off between latent code information and sample quality.

- [ ] **Tomczak, J. M. & Welling, M. (2018).** *VAE with a VampPrior.*  
  Proposed a more flexible prior (VampPrior) for VAEs, which leads to improved generative performance by better matching the aggregated posterior.
